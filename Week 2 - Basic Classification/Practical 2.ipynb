{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Data Mining 2016 - Assignment 2: Know your Models\n",
    "\n",
    "In the first skills class, we mainly focussed on interpreting the data and forming hypotheses and expectations that rose from some natural intuition about the data. You examined and analysed the individual features (with histograms) and the combination of pairs of features (with scatter plots) to see to what extent these features separated the two classes. In this class we are diving into applying basic algorithms of classification to these problems to confirm (or reject) your own predictions.\n",
    "\n",
    "\n",
    "## 0.1 - Prepare your Data\n",
    "\n",
    "Because you have now developed some knowledge about the contents of the dataset we provided last time, it is best to use the same sets to capatialize on that. To really get a sense of the differences between several classification tasks, we advise that you again test for **more than one** dataset.\n",
    "\n",
    "## 0.2 - Refresher on the Models\n",
    "\n",
    "Today we will use $k$-nearest neighbours, as well as a decision tree classifier (J48) for classification.\n",
    "\n",
    "### $k$-NN\n",
    "Recall that every **feature vector** can be represented as a point in a graph. The basic algorithm behind $k$-NN just stores the points that you provide in the training phase. As such, the model has a sort of memory of the position of each point in its $N$-dimensional space (where $N$ is the amount of features, so 3 features could be represented in a 3-dimension space). Because it also knows the labels of these points (instances), it can predict new (to the algorithm unlabelled) instance in the test phase. Using a naive method for computation, this is done by measuring the distances between this new point, and every point that the model knows about. It then selects the top $k$ (therefore $k$-nearest) closest / nearest neighbours to this one point. For those interested, this distance calculation is done using [Euclidean Distances](https://en.wikipedia.org/wiki/Euclidean_distance) (for continuous variables at least). Basically it uses the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem) on the 'line' between the origin (0, ..., 0) and point 1, and the origin and point 2, so it calculates the unknown 'line' between point 1 and 2.\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "J48 is an implementation of the C4.5 algorithm (which is an improvement on the ID3 algorithm) for constructing decision trees. It tries to determine certain values for each feature on which to make a decision (older than 50, is male), where preferably, the values that split the space most effectively between class labels will be highest in the tree. As you saw in the lecture, you can do this visually by trying to draw as few lines as possible to isolate the instances of only one label. If you're intersted in the more formal explanation, how entropy and information gain are used to make these decisions, we recommend reading [this](http://www.csse.monash.edu.au/courseware/cse5230/2004/assets/decisiontreesTute.pdf).\n",
    "\n",
    "## 1 - How to Evaluate\n",
    "\n",
    "Understanding how to score these algorithms on your classification task is an important part of checking if your hypotheses were correct and if it's actually doing what you want it to do.\n",
    "\n",
    "| nr | label | prediction |\n",
    "| -- | ----- | ---------- |\n",
    "| 1  | dog   | cat        |\n",
    "| 2  | cat   | dog        |\n",
    "| 3  | ball  | ball       |\n",
    "| 4  | dog   | dog        |\n",
    "| 5  | cat   | cat        |\n",
    "| 6  | ball  | cat        |\n",
    "| 7  | dog   | dog        |\n",
    "| 8  | dog   | cat        |\n",
    "| 9  | dog   | cat        |\n",
    "| 10 | cat   | cat        |\n",
    "| 11 | cat   | cat        |\n",
    "| 12 | cat   | cat        |\n",
    "| 13 | cat   | cat        |\n",
    "| 14 | cat   | dog        |\n",
    "| 15 | ball  | ball       |\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Given the classification result above, compute the accuracy of the classifier.\n",
    "2. Make a confusion matrix of the results.\n",
    "3. Do the results make sense some way, if you look at the distribution of the labels?\n",
    "4. What would be a baseline score for this task?\n",
    "5. How well does the classifier do compared to the baseline?\n",
    "6. Do you think it has actually learned anything?\n",
    "7. Can you already come up with a reason why the classifier performs the way it does?\n",
    "\n",
    "## 2 - Actually Doing\n",
    "\n",
    "There is a huge range of factors that might affect the performance of your classifier. Some are inherent to the data, and hard to find a good solution for, others are due to your own design choices. It's very important to understand how certain classifier behave under different circumstances, and although there are some best practices that will be thaught in these practicals, most of this you can only overcome emperically. \n",
    "\n",
    "- Select two or more classification problems (i.e. datasets).\n",
    "- Open up WEKA.\n",
    "\n",
    "In this part we will focus on running the two classifiers we discussed, and interpreting the results under different choices. Repeat these steps **for each different setting** in the questions below:\n",
    "\n",
    "\n",
    "- Load the data (resets any changes you made).\n",
    "- Make sure to remove any non-numeric or nominal features.\n",
    "- Do a Task (from the list below).\n",
    "- Click the Classify tab.\n",
    "- Click 'use training set'.\n",
    "- Underneath 'more options' select the feature you want to predict (i.e. make the class label).\n",
    "- Run ZeroR (majority baseline).\n",
    "- Under 'Choose' (left top) select -> lazy -> IBK, run.\n",
    "- Under 'Choose' (left top) select -> tree -> J48, run.\n",
    "- Write down the results (accuracy and confusion matrix).\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "For each feature combination you determine the accuracy and the confusion matrix. Explain the results for yourself in terms of the nature of the feature you used.\n",
    "\n",
    "8. Remove all but the ONE feature you think is LEAST informative for the classification problem.\n",
    "9. Remove all but the TWO features you think are LEAST informative for the classification problem.\n",
    "10. Remove all but the ONE feature you think is MOST informative for the classification problem.\n",
    "11. Remove all but the TWO features you think are MOST informative for the classification problem.\n",
    "12. Run on all the features.\n",
    "\n",
    "Given that you've now ascertained the performance of these classifiers, compare your answers for all the settings and answer this:\n",
    "\n",
    "13. Why are there differences in the performance of both classifiers? Explain the answer in terms of decision boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
